{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant modules\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf \n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing shakespeare text\n",
    "f = open('data/shakespeare.txt', 'r')\n",
    "\n",
    "lines = []\n",
    "for line in f:\n",
    "    lines.append(line)\n",
    "\n",
    "#Corpus is the raw_data\n",
    "corpus = [line[:-2] for line in lines if len(line.split()) > 1]\n",
    "\n",
    "#adds all the text in the document to the string corpus-concat\n",
    "corpus_concat = \"\"\n",
    "for line in corpus:\n",
    "    corpus_concat+=line + \" \"\n",
    "corpus_concat = re.sub(r'[^\\w]',' ', corpus_concat).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of sequences: 18419\n"
     ]
    }
   ],
   "source": [
    "#Makes a list of subsequences and test labels\n",
    "subsequences = []\n",
    "test_labels = []\n",
    "#Sequence buffer is the 'n' that determines the difference between successive sequences\n",
    "sequence_buffer = 5\n",
    "char_size = 40\n",
    "\n",
    "for i in range(0, len(corpus_concat)-1, sequence_buffer):\n",
    "    #Ensures that the test_label is not empty\n",
    "    if (corpus_concat[i+char_size:i+(char_size+1)]!=''):\n",
    "        subsequences.append(corpus_concat[i:i+char_size])\n",
    "        test_labels.append(corpus_concat[i+char_size:i+(char_size+1)])\n",
    "        \n",
    "#Makes a list of all the characters involved and makes a dictionary out of them\n",
    "chars = sorted(list(set(corpus_concat)))\n",
    "forward_mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "#So we can have an inverse\n",
    "backward_mapping = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print (\"Num of sequences:\", len(subsequences))\n",
    "n_patterns = len(subsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  27\n"
     ]
    }
   ],
   "source": [
    "#Encoding the labels\n",
    "labels_encoded = [forward_mapping[lab] for lab in test_labels]\n",
    "\n",
    "#Encoding the subsequences\n",
    "seq_list = list()\n",
    "for sequence in subsequences:\n",
    "    encoded_seq = [forward_mapping[char] for char in sequence]\n",
    "    seq_list.append(encoded_seq)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(chars)\n",
    "print (\"vocab size: \", vocab_size)\n",
    "\n",
    "#Changing from normal array/list to numpy array\n",
    "normal_seq_list = []\n",
    "for seq in seq_list:\n",
    "    normal_seq_list.append(np.array(seq))\n",
    "normal_seq_list = np.array(normal_seq_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (18419, 40, 27)\n"
     ]
    }
   ],
   "source": [
    "#Using to_categorical to prepare for sending through neural network.\n",
    "seq_input = [keras.utils.to_categorical(seq, num_classes=vocab_size) for seq in normal_seq_list]\n",
    "X = np.array(seq_input)\n",
    "y = keras.utils.to_categorical(labels_encoded, num_classes=vocab_size)\n",
    "\n",
    "print (\"Shape of X:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18419/18419 [==============================] - 47s 3ms/step - loss: 2.6778 - acc: 0.2345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x134e42da0>"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting Model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(200, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "#Using crossentropy and adam \n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#ADJUST NUMBER OF EPOCHS\n",
    "model.fit(X, y, epochs=1, batch_size = 50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s testy sick men when their deaths be ne |\n"
     ]
    }
   ],
   "source": [
    "#Starting at random sequence in X_new\n",
    "rand = random.randint(0, len(X_new))\n",
    "pattern = X_new[rand]\n",
    "\n",
    "#getting indices of highest elements and printing corresponding string\n",
    "nums = ([np.argmax(x) for x in pattern])\n",
    "print (''.join([backward_mapping[value] for value in nums]), \"|\")\n",
    "\n",
    "for i in range (1000):\n",
    "    #Making sure we have the right size\n",
    "    to_predict = numpy.reshape(pattern[-char_size:], (1, char_size, vocab_size))\n",
    "    prediction = model.predict(to_predict, verbose=0)\n",
    "    #Getting index of largest element\n",
    "    index = numpy.argmax(prediction)\n",
    "    #Changing to categorical because that is how 'pattern' elements are stored\n",
    "    y = keras.utils.to_categorical(index, num_classes=vocab_size)\n",
    "    pattern = np.vstack((pattern, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s testy sick men when their deaths be ne the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the |\n"
     ]
    }
   ],
   "source": [
    "nums = ([np.argmax(x) for x in pattern])\n",
    "string = [backward_mapping[value] for value in nums]\n",
    "\n",
    "print(''.join(string), \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
