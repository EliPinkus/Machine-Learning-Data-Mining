{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/data.txt', delimiter=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_U(Ui, Yij, Vj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta*(-(Yij-np.dot(Ui, Vj))*Vj.T + reg*Ui.T)\n",
    "\n",
    "def grad_V(Vj, Yij, Ui, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta*(-(Yij-np.dot(Vj,Ui))*Ui.T+reg*Vj.T)\n",
    "\n",
    "\n",
    "\n",
    "def get_err(U, V, Y, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    sum_err = 0\n",
    "    for element in Y:\n",
    "        i = np.int(element[0])-1\n",
    "        j = np.int(element[1])-1\n",
    "        sum_err += (element[2]-np.dot(U[i],V[j]))**2\n",
    "    return (reg/2 * ((np.linalg.norm(U, 'fro')+np.linalg.norm(V, 'fro'))) + sum_err)/len(Y)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=100):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    #Creating U and V matrices\n",
    "    U = np.random.rand(M, K)-0.5\n",
    "    V = np.random.rand(N, K)-0.5\n",
    "    delta = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        prev_error = get_err(U, V, Y, 0)\n",
    "        \n",
    "        #Shuffling Y matrix\n",
    "        Y_shuffled = Y[np.random.permutation(np.arange(len(Y)))]\n",
    "        #Looping through the 2 dimensions of Y\n",
    "        for element in Y_shuffled:\n",
    "            i = np.int(element[0])-1\n",
    "            j = np.int(element[1])-1\n",
    "            #Computing gradient and descending along it. Trivial.\n",
    "            cur_grad_u = grad_U(U[i], element[2], V[j], reg, eta)\n",
    "            cur_grad_v = grad_V(V[j], element[2], U[i], reg, eta)\n",
    "            U[i] = U[i]-cur_grad_u\n",
    "            V[j] = V[j]-cur_grad_v\n",
    "            \n",
    "        cur_error = get_err(U, V, Y, 0)\n",
    "        \n",
    "        if (epoch==0):\n",
    "            delta = np.abs(cur_error-prev_error) # Setting loss for the first epoch.\n",
    "        elif (np.abs((cur_error-prev_error))/delta<= eps):\n",
    "            print(epoch)\n",
    "            break\n",
    "    #Returning Error from final model \n",
    "    \n",
    "    return U, V, get_err(U, V, Y, 0)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1682\n",
    "M = 943\n",
    "K = 20\n",
    "reg = 0\n",
    "eta = 0.003\n",
    "U, V, err = train_model(M, N, K, eta, reg, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_U_bias(Ui, Yij, Vj, Ai, Bj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input Ui (the ith row of U), a training point Yij, the column\n",
    "    vector Vj (jth column of V^T), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Ui multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta*(-(Yij-np.dot(Ui, Vj) - Ai-Bj)*Vj.T + reg*Ui.T)\n",
    "\n",
    "def grad_V_bias(Vj, Yij, Ui, Ai, Bj, reg, eta):\n",
    "    \"\"\"\n",
    "    Takes as input the column vector Vj (jth column of V^T), a training point Yij,\n",
    "    Ui (the ith row of U), reg (the regularization parameter lambda),\n",
    "    and eta (the learning rate).\n",
    "\n",
    "    Returns the gradient of the regularized loss function with\n",
    "    respect to Vj multiplied by eta.\n",
    "    \"\"\"\n",
    "    return eta*(-(Yij-np.dot(Vj,Ui)-Ai-Bj)*Ui.T+reg*Vj.T)\n",
    "\n",
    "def grad_A_bias(Ui, Yij, Vj, Ai, Bj, reg, eta):\n",
    "    '''Finds gradient of A vector'''\n",
    "    return eta*(-(Yij-np.dot(Ui, Vj) - Ai - Bj))\n",
    "\n",
    "def grad_B_bias(Vj, Yij, Ui, Ai, Bj, reg, eta):\n",
    "    '''Finds gradient of B vector'''\n",
    "    return eta*(-(Yij-np.dot(Ui, Vj) - Ai - Bj))\n",
    "\n",
    "\n",
    "def get_err_bias(U, V, Y, A, B, reg=0.0):\n",
    "    \"\"\"\n",
    "    Takes as input a matrix Y of triples (i, j, Y_ij) where i is the index of a user,\n",
    "    j is the index of a movie, and Y_ij is user i's rating of movie j and\n",
    "    user/movie matrices U and V.\n",
    "\n",
    "    Returns the mean regularized squared-error of predictions made by\n",
    "    estimating Y_{ij} as the dot product of the ith row of U and the jth column of V^T.\n",
    "    \"\"\"\n",
    "    sum_err = 0\n",
    "    for element in Y:\n",
    "        i = np.int(element[0])-1\n",
    "        j = np.int(element[1])-1\n",
    "        sum_err += (element[2]-np.dot(U[i],V[j])-A[i]-B[j])**2\n",
    "    return (reg/2 * ((np.linalg.norm(U, 'fro')+np.linalg.norm(V, 'fro'))) + sum_err)/len(Y)\n",
    "\n",
    "\n",
    "\n",
    "def train_model_bias(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=10):\n",
    "    \"\"\"\n",
    "    Given a training data matrix Y containing rows (i, j, Y_ij)\n",
    "    where Y_ij is user i's rating on movie j, learns an\n",
    "    M x K matrix U and N x K matrix V such that rating Y_ij is approximated\n",
    "    by (UV^T)_ij.\n",
    "\n",
    "    Uses a learning rate of <eta> and regularization of <reg>. Stops after\n",
    "    <max_epochs> epochs, or once the magnitude of the decrease in regularized\n",
    "    MSE between epochs is smaller than a fraction <eps> of the decrease in\n",
    "    MSE after the first epoch.\n",
    "\n",
    "    Returns a tuple (U, V, err) consisting of U, V, and the unregularized MSE\n",
    "    of the model.\n",
    "    \"\"\"\n",
    "    #Creating U and V matrices\n",
    "    U = np.random.rand(M, K)-0.5\n",
    "    V = np.random.rand(N, K)-0.5\n",
    "    \n",
    "    #Creating A and B vectors\n",
    "    A = np.random.rand(M, 1)-0.5\n",
    "    B = np.random.rand(N, 1)-0.5\n",
    "    delta = 0\n",
    "    for epoch in range(max_epochs):\n",
    "        print (\"Epoch \", epoch)\n",
    "        prev_error = get_err_bias(U, V, Y, A, B, 0)\n",
    "        \n",
    "        #Shuffling Y matrix\n",
    "        Y_shuffled = Y[np.random.permutation(np.arange(len(Y)))]\n",
    "        #Looping through the 2 dimensions of Y\n",
    "        for element in Y_shuffled:\n",
    "            i = np.int(element[0])-1\n",
    "            j = np.int(element[1])-1\n",
    "            #Computing gradient and descending along it. Trivial.\n",
    "            cur_grad_u = grad_U_bias(U[i], element[2], V[j], A[i], B[j], reg, eta)\n",
    "            cur_grad_v = grad_V_bias(V[j], element[2], U[i], A[i], B[j], reg, eta)\n",
    "            cur_grad_a = grad_A_bias(U[i], element[2], V[j], A[i], B[j], reg, eta)\n",
    "            cur_grad_b = grad_B_bias(V[j], element[2], U[i], A[i], B[j], reg, eta)\n",
    "            U[i] = U[i]-cur_grad_u\n",
    "            V[j] = V[j]-cur_grad_v\n",
    "            A[i] = A[i]-cur_grad_a\n",
    "            B[j] = B[j]-cur_grad_b\n",
    "        cur_error = get_err_bias(U, V, Y, A, B, 0)\n",
    "        \n",
    "        if (epoch==0):\n",
    "            delta = np.abs(cur_error-prev_error) # Setting loss for the first epoch.\n",
    "        elif (np.abs((cur_error-prev_error))/delta<= eps):\n",
    "            print(epoch)\n",
    "            break\n",
    "        print (cur_error)\n",
    "    #Returning Error from final model \n",
    "    \n",
    "    return U, V, A, B, get_err_bias(U, V, Y, A, B, 0)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "[ 3.255521]\n",
      "Epoch  1\n",
      "[ 1.88259886]\n",
      "Epoch  2\n",
      "[ 1.48824902]\n",
      "Epoch  3\n",
      "[ 1.29447507]\n",
      "Epoch  4\n",
      "[ 1.17360837]\n",
      "Epoch  5\n",
      "[ 1.08903238]\n",
      "Epoch  6\n",
      "[ 1.02561708]\n",
      "Epoch  7\n",
      "[ 0.97577515]\n",
      "Epoch  8\n",
      "[ 0.93515601]\n",
      "Epoch  9\n",
      "[ 0.90117285]\n"
     ]
    }
   ],
   "source": [
    "N = 1682\n",
    "M = 943\n",
    "K = 20\n",
    "reg = 0\n",
    "eta = 0.003\n",
    "U, V, A, B, err = train_model_bias(M, N, K, eta, reg, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
